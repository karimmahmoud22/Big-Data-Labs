{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Karim Mahmoud Kamal\n",
        "# Sec: 2\n",
        "# B.N: 12\n",
        "\n",
        "# Mustafa Mahmoud Hamada\n",
        "# Sec: 2\n",
        "# B.N: 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Af8PYD3Cpj3",
        "outputId": "86b9ca83-45d0-44b3-e80d-4818604e5b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XyE_BQLlFG7L"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5rFS3byHFN3X"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pUqy20ipF3Ul"
      },
      "outputs": [],
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Card Amount Count\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FQmWgbp1JYiy"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"/content/in.csv\", header=False, inferSchema=True)\n",
        "\n",
        "# Convert the DataFrame to an RDD of tuples\n",
        "parsedLines = df.rdd.map(lambda row: (row[0], row[1]))\n",
        "\n",
        "# Performing sum aggregation by key\n",
        "customerAmounts = parsedLines.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Sort the output by key\n",
        "sortedCustomerAmounts = customerAmounts.sortByKey()\n",
        "\n",
        "# Repartition to a single partition to have only one output file\n",
        "sortedCustomerAmounts = sortedCustomerAmounts.coalesce(1)\n",
        "\n",
        "# Save the sorted output into a single file\n",
        "sortedCustomerAmounts.saveAsTextFile(\"/content/out_final.txt\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnSZNFKNKRaz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
